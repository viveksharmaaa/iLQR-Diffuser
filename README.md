# iLQR-Diffuser: Diffusion Trajectory Generation with iLQR-Based Dynamic Feasibility
In this project, we propose a novel diffusion-based trajectory generation architecture that produces **dynamically feasible** robot trajectories by integrating a **multiple-shooting iLQR projection** step into both training and inference of a diffusion transformer.
Our approach combines a diffusion transformer with a multiple-shooting iLQR projection layer that enforces consistency with robot dynamics and actuation limits.

## Overview

Diffusion models are inherently stochastic and unconstrained. As a result, trajectories generated by standard diffusion models **do not, in general, satisfy the robot’s equations of motion** or input constraints.

When such *dynamically infeasible* trajectories are executed on a real robot or simulator, the system quickly diverges from the predicted motion, often leading to task failure or safety violations.  
Prior work typically mitigates this issue by **frequent replanning** or closed-loop tracking, which treats infeasibility as a downstream correction problem rather than addressing its root cause.
In contrast, we directly enforce **dynamic feasibility at the trajectory generation level**.

---
## Theory

We assume to have a black-box discrete-time simulator $f$ of a robot state 

$$s_{t+1} = f(s_t, a_t)$$

controlled by actions $a_t \in \mathcal A$.
A trajectory $\tau = \\{s_0, s_1, ...\\}$ is **admissible** if each $s_{t+1}$ is reachable from its predecessor $s_t$ with an action $a_t \in \mathcal{A}$. In other words $s_{t+1}$ must be in the **reachable set** $\mathcal{R}(s_t)$ of its predecessor $s_t$ where

$$\mathcal{R}(s_t) = \\{ f(s_t, a)\ \text{for all}\ a \in \mathcal{A} \\}.$$

Many planning algorithms produce collision-free or task-feasible **state sequences** that do not satisfy this condition. To convert such sequences into dynamically admissible trajectories, we use **multiple-shooting iterative LQR (iLQR)**.

---

### Trajectory Optimization Problem

Given:
- initial state $s_0$,
- planning horizon $T$,
- reference  (possibly dynamically infeasible) trajectory $\{{s'}_t\}_{t=0}^T$ which is the outcome of diffusion model,

we solve the finite-horizon optimal control problem

$$\min_{\{s_t,a_t\}}\sum_{t=0}^{T-1} \ell(s_t, a_t) + \ell_T(s_T)$$
subject to
$$s_{t+1} = f(s_t, a_t), s.t. a_t \in \mathcal{A}, \text{and }  s_0 \text{ is fixed}.$$

We use quadratic tracking costs of the form

$$\ell(s_t,a_t) = \tfrac{1}{2}\|s_t - \bar{s}_t\|_Q^2 + \tfrac{1}{2}\|a_t\|_R^2,
\qquad
\ell_T(s_T) = \tfrac{1}{2}\|s_T - \bar{s}_T\|_{Q_T}^2.
$$

---

### Multiple-Shooting Formulation

Unlike single-shooting iLQR, which optimizes only over control inputs, **multiple shooting** treats intermediate states $\{s_t\}$ as decision variables.  
Dynamics consistency is enforced via **defect constraints**

$$d_t = s_{t+1} - f(s_t, a_t).$$

The optimization problem becomes

$$\min_{\{s_t,a_t\}} \sum_{t=0}^{T-1} \ell(s_t,a_t) + \ell_T(s_T)
\quad \text{s.t.} \quad
d_t = 0,\;\; a_t \in \mathcal{A}.
$$

In practice, especially when $f$ is a black-box simulator, we enforce dynamics using a penalty

$$
J_{\text{MS}} =
\sum_{t=0}^{T-1} \ell(s_t,a_t) + \ell_T(s_T)+ \sum_{t=0}^{T-1} \frac{\rho}{2} \|d_t\|^2.
$$

This formulation improves numerical stability, allows poor initial guesses, and scales better to long horizons and highly nonlinear dynamics.

---

## iLQR Optimization

At each iteration, the dynamics are locally linearized around the nominal trajectory.  
Since the system dynamics are provided only through a **black-box discrete-time simulator**, the Jacobians are estimated using **finite differences**:


$$
\delta s_{t+1} \approx A_t \delta s_t + B_t \delta a_t,
\quad
A_t \approx \frac{\partial f}{\partial s}, \;
B_t \approx \frac{\partial f}{\partial a}.
$$

Concretely, for a small perturbation $\varepsilon > 0$, the Jacobians are computed as


$$A_t^{(i)} \approx \frac{f(s_t + \varepsilon e_i, a_t) - f(s_t, a_t)}{\varepsilon},
\qquad
B_t^{(j)} \approx \frac{f(s_t, a_t + \varepsilon e_j) - f(s_t, a_t)}{\varepsilon},
$$

where $e_i$ and $e_j$ denote standard basis vectors in the state and action spaces, respectively.

The stage and terminal costs are then quadratized around the nominal trajectory, yielding a local linear–quadratic approximation that is solved using the standard iLQR backward–forward pass.


The resulting local LQR problem is solved using a backward Riccati pass, yielding affine feedback updates

$$\delta a_t = k_t + K_t \delta s_t.$$

A forward rollout with line search is then performed, with actions projected onto the admissible set $\mathcal{A}$.

---

### Result

The final output is a **smooth, dynamically feasible trajectory**


$$\tau = \{(s_t, a_t)\}_{t=0}^{T-1}$$

that satisfies simulator dynamics and action constraints. This trajectory can be:
- executed directly,
- used as a warm-start for MPC,
- or refined further under additional constraints (e.g., safety via CBFs).

## Organization

- `code` : our implementation of iLQR with diffusion transformers, and trained models.

## Acknowledgments

Our code is largely modified from source code based on  [DDAT](https://github.com/labicon/DDAT).


